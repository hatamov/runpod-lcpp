# These are requirements are from llama-cpp-python (main + server)
# https://github.com/abetlen/llama-cpp-python/blob/main/pyproject.toml
typing-extensions>=4.5.0
numpy>=1.20.0
diskcache>=5.6.1
jinja2>=2.11.3
uvicorn>=0.22.0
fastapi>=0.100.0
pydantic-settings>=2.0.1
sse-starlette>=1.6.1
starlette-context>=0.3.6,<0.4
PyYAML>=5.1

# Other requirements:
