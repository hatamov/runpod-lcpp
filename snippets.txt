Some snippets for testing

```
python3 /src/handler.py  --rp_serve_api --rp_api_host='0.0.0.0'

curl -X POST http://localhost:8000/runsync \
    -H "Content-Type: application/json" \
    --data '{"input":{"openai_route": "/v1/completions", "openai_input":{"prompt": "Once upon a time",  "n_predict": 20, "stream":true}}}'

payloads:

{
  "input": {
    "openai_route": "/v1/completions",
    "openai_input": {
      "prompt": "Once upon a time",
      "n_predict": 20,
      "model": "astronomer/Llama-3-8B-Instruct-GPTQ-8-Bit",
      "stream": true
    }
  }
}

{
  "input": {
    "openai_route": "/v1/completions",
    "openai_input": {
      "prompt": "Once upon a time",
      "n_predict": 20,
      "stream": true
    }
  }
}

{
  "input": {
    "openai_route": "/v1/models",
    "openai_input": {
      "prompt": "Once upon a time",
      "n_predict": 20
    }
  }
}

{
  "input": {
    "env": {
      "INITIAL_SERVER": "EXL2"
    },
    "update_env": true,
    "prompt": "!rs"
  }
}

### USAGE LLAMA
root@b017ef68d32c:/tabbyAPI# /venv-lcpp/bin/python  -m llama_cpp.server  --help
usage: __main__.py [-h] [--model MODEL] [--model_alias MODEL_ALIAS] [--n_gpu_layers N_GPU_LAYERS] [--split_mode SPLIT_MODE] [--main_gpu MAIN_GPU] [--tensor_split [TENSOR_SPLIT ...]]
                   [--vocab_only VOCAB_ONLY] [--use_mmap USE_MMAP] [--use_mlock USE_MLOCK] [--kv_overrides [KV_OVERRIDES ...]] [--rpc_servers RPC_SERVERS] [--seed SEED] [--n_ctx N_CTX]
                   [--n_batch N_BATCH] [--n_threads N_THREADS] [--n_threads_batch N_THREADS_BATCH] [--rope_scaling_type ROPE_SCALING_TYPE] [--rope_freq_base ROPE_FREQ_BASE]
                   [--rope_freq_scale ROPE_FREQ_SCALE] [--yarn_ext_factor YARN_EXT_FACTOR] [--yarn_attn_factor YARN_ATTN_FACTOR] [--yarn_beta_fast YARN_BETA_FAST]
                   [--yarn_beta_slow YARN_BETA_SLOW] [--yarn_orig_ctx YARN_ORIG_CTX] [--mul_mat_q MUL_MAT_Q] [--logits_all LOGITS_ALL] [--embedding EMBEDDING] [--offload_kqv OFFLOAD_KQV]
                   [--flash_attn FLASH_ATTN] [--last_n_tokens_size LAST_N_TOKENS_SIZE] [--lora_base LORA_BASE] [--lora_path LORA_PATH] [--numa NUMA] [--chat_format CHAT_FORMAT]
                   [--clip_model_path CLIP_MODEL_PATH] [--cache CACHE] [--cache_type CACHE_TYPE] [--cache_size CACHE_SIZE] [--hf_tokenizer_config_path HF_TOKENIZER_CONFIG_PATH]
                   [--hf_pretrained_model_name_or_path HF_PRETRAINED_MODEL_NAME_OR_PATH] [--hf_model_repo_id HF_MODEL_REPO_ID] [--draft_model DRAFT_MODEL]
                   [--draft_model_num_pred_tokens DRAFT_MODEL_NUM_PRED_TOKENS] [--type_k TYPE_K] [--type_v TYPE_V] [--verbose VERBOSE] [--host HOST] [--port PORT] [--ssl_keyfile SSL_KEYFILE]
                   [--ssl_certfile SSL_CERTFILE] [--api_key API_KEY] [--interrupt_requests INTERRUPT_REQUESTS] [--disable_ping_events DISABLE_PING_EVENTS] [--root_path ROOT_PATH]
                   [--config_file CONFIG_FILE]

ðŸ¦™ Llama.cpp python server. Host your own LLMs!ðŸš€

options:
  -h, --help            show this help message and exit
  --model MODEL         The path to the model to use for generating completions.
  --model_alias MODEL_ALIAS
                        The alias of the model to use for generating completions.
  --n_gpu_layers N_GPU_LAYERS
                        The number of layers to put on the GPU. The rest will be on the CPU. Set -1 to move all to GPU.
  --split_mode SPLIT_MODE
                        The split mode to use. (default: 1)
  --main_gpu MAIN_GPU   Main GPU to use.
  --tensor_split [TENSOR_SPLIT ...]
                        Split layers across multiple GPUs in proportion.
  --vocab_only VOCAB_ONLY
                        Whether to only return the vocabulary.
  --use_mmap USE_MMAP   Use mmap. (default: True)
  --use_mlock USE_MLOCK
                        Use mlock. (default: True)
  --kv_overrides [KV_OVERRIDES ...]
                        List of model kv overrides in the format key=type:value where type is one of (bool, int, float). Valid true values are (true, TRUE, 1), otherwise false.
  --rpc_servers RPC_SERVERS
                        comma seperated list of rpc servers for offloading
  --seed SEED           Random seed. -1 for random. (default: 4294967295)
  --n_ctx N_CTX         The context size. (default: 2048)
  --n_batch N_BATCH     The batch size to use per eval. (default: 512)
  --n_threads N_THREADS
                        The number of threads to use. Use -1 for max cpu threads (default: 48)
  --n_threads_batch N_THREADS_BATCH
                        The number of threads to use when batch processing. Use -1 for max cpu threads (default: 96)
  --rope_scaling_type ROPE_SCALING_TYPE
  --rope_freq_base ROPE_FREQ_BASE
                        RoPE base frequency
  --rope_freq_scale ROPE_FREQ_SCALE
                        RoPE frequency scaling factor
  --yarn_ext_factor YARN_EXT_FACTOR
  --yarn_attn_factor YARN_ATTN_FACTOR
  --yarn_beta_fast YARN_BETA_FAST
  --yarn_beta_slow YARN_BETA_SLOW
  --yarn_orig_ctx YARN_ORIG_CTX
  --mul_mat_q MUL_MAT_Q
                        if true, use experimental mul_mat_q kernels (default: True)
  --logits_all LOGITS_ALL
                        Whether to return logits. (default: True)
  --embedding EMBEDDING
                        Whether to use embeddings. (default: True)
  --offload_kqv OFFLOAD_KQV
                        Whether to offload kqv to the GPU. (default: True)
  --flash_attn FLASH_ATTN
                        Whether to use flash attention.
  --last_n_tokens_size LAST_N_TOKENS_SIZE
                        Last n tokens to keep for repeat penalty calculation. (default: 64)
  --lora_base LORA_BASE
                        Optional path to base model, useful if using a quantized base model and you want to apply LoRA to an f16 model.
  --lora_path LORA_PATH
                        Path to a LoRA file to apply to the model.
  --numa NUMA           Enable NUMA support.
  --chat_format CHAT_FORMAT
                        Chat format to use.
  --clip_model_path CLIP_MODEL_PATH
                        Path to a CLIP model to use for multi-modal chat completion.
  --cache CACHE         Use a cache to reduce processing times for evaluated prompts.
  --cache_type CACHE_TYPE
                        The type of cache to use. Only used if cache is True. (default: ram)
  --cache_size CACHE_SIZE
                        The size of the cache in bytes. Only used if cache is True. (default: 2147483648)
  --hf_tokenizer_config_path HF_TOKENIZER_CONFIG_PATH
                        The path to a HuggingFace tokenizer_config.json file.
  --hf_pretrained_model_name_or_path HF_PRETRAINED_MODEL_NAME_OR_PATH
                        The model name or path to a pretrained HuggingFace tokenizer model. Same as you would pass to AutoTokenizer.from_pretrained().
  --hf_model_repo_id HF_MODEL_REPO_ID
                        The model repo id to use for the HuggingFace tokenizer model.
  --draft_model DRAFT_MODEL
                        Method to use for speculative decoding. One of (prompt-lookup-decoding).
  --draft_model_num_pred_tokens DRAFT_MODEL_NUM_PRED_TOKENS
                        Number of tokens to predict using the draft model. (default: 10)
  --type_k TYPE_K       Type of the key cache quantization.
  --type_v TYPE_V       Type of the value cache quantization.
  --verbose VERBOSE     Whether to print debug information. (default: True)
  --host HOST           Listen address (default: localhost)
  --port PORT           Listen port (default: 8000)
  --ssl_keyfile SSL_KEYFILE
                        SSL key file for HTTPS
  --ssl_certfile SSL_CERTFILE
                        SSL certificate file for HTTPS
  --api_key API_KEY     API key for authentication. If set all requests need to be authenticated.
  --interrupt_requests INTERRUPT_REQUESTS
                        Whether to interrupt requests when a new request is received. (default: True)
  --disable_ping_events DISABLE_PING_EVENTS
                        Disable EventSource pings (may be needed for some clients).
  --root_path ROOT_PATH
                        The root path for the server. Useful when running behind a reverse proxy.
  --config_file CONFIG_FILE
                        Path to a config file to load.



## USAGE VLLM:
python3 -m vllm.entrypoints.openai.api_server --help

usage: api_server.py [-h] [--host HOST] [--port PORT] [--uvicorn-log-level {debug,info,warning,error,critical,trace}] [--allow-credentials] [--allowed-origins ALLOWED_ORIGINS]
                     [--allowed-methods ALLOWED_METHODS] [--allowed-headers ALLOWED_HEADERS] [--api-key API_KEY] [--lora-modules LORA_MODULES [LORA_MODULES ...]]
                     [--chat-template CHAT_TEMPLATE] [--response-role RESPONSE_ROLE] [--ssl-keyfile SSL_KEYFILE] [--ssl-certfile SSL_CERTFILE] [--ssl-ca-certs SSL_CA_CERTS]
                     [--ssl-cert-reqs SSL_CERT_REQS] [--root-path ROOT_PATH] [--middleware MIDDLEWARE] [--model MODEL] [--tokenizer TOKENIZER] [--skip-tokenizer-init] [--revision REVISION]
                     [--code-revision CODE_REVISION] [--tokenizer-revision TOKENIZER_REVISION] [--tokenizer-mode {auto,slow}] [--trust-remote-code] [--download-dir DOWNLOAD_DIR]
                     [--load-format {auto,pt,safetensors,npcache,dummy,tensorizer}] [--dtype {auto,half,float16,bfloat16,float,float32}] [--kv-cache-dtype {auto,fp8}]
                     [--quantization-param-path QUANTIZATION_PARAM_PATH] [--max-model-len MAX_MODEL_LEN] [--guided-decoding-backend {outlines,lm-format-enforcer}] [--worker-use-ray]
                     [--pipeline-parallel-size PIPELINE_PARALLEL_SIZE] [--tensor-parallel-size TENSOR_PARALLEL_SIZE] [--max-parallel-loading-workers MAX_PARALLEL_LOADING_WORKERS]
                     [--ray-workers-use-nsight] [--block-size {8,16,32}] [--enable-prefix-caching] [--use-v2-block-manager] [--num-lookahead-slots NUM_LOOKAHEAD_SLOTS] [--seed SEED]
                     [--swap-space SWAP_SPACE] [--gpu-memory-utilization GPU_MEMORY_UTILIZATION] [--num-gpu-blocks-override NUM_GPU_BLOCKS_OVERRIDE]
                     [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS] [--max-num-seqs MAX_NUM_SEQS] [--max-logprobs MAX_LOGPROBS] [--disable-log-stats]
                     [--quantization {aqlm,awq,fp8,gptq,squeezellm,gptq_marlin,marlin,None}] [--enforce-eager] [--max-context-len-to-capture MAX_CONTEXT_LEN_TO_CAPTURE]
                     [--max-seq_len-to-capture MAX_SEQ_LEN_TO_CAPTURE] [--disable-custom-all-reduce] [--tokenizer-pool-size TOKENIZER_POOL_SIZE] [--tokenizer-pool-type TOKENIZER_POOL_TYPE]
                     [--tokenizer-pool-extra-config TOKENIZER_POOL_EXTRA_CONFIG] [--enable-lora] [--max-loras MAX_LORAS] [--max-lora-rank MAX_LORA_RANK]
                     [--lora-extra-vocab-size LORA_EXTRA_VOCAB_SIZE] [--lora-dtype {auto,float16,bfloat16,float32}] [--max-cpu-loras MAX_CPU_LORAS] [--fully-sharded-loras]
                     [--device {auto,cuda,neuron,cpu}] [--image-input-type {pixel_values,image_features}] [--image-token-id IMAGE_TOKEN_ID] [--image-input-shape IMAGE_INPUT_SHAPE]
                     [--image-feature-size IMAGE_FEATURE_SIZE] [--scheduler-delay-factor SCHEDULER_DELAY_FACTOR] [--enable-chunked-prefill] [--speculative-model SPECULATIVE_MODEL]
                     [--num-speculative-tokens NUM_SPECULATIVE_TOKENS] [--speculative-max-model-len SPECULATIVE_MAX_MODEL_LEN] [--ngram-prompt-lookup-max NGRAM_PROMPT_LOOKUP_MAX]
                     [--ngram-prompt-lookup-min NGRAM_PROMPT_LOOKUP_MIN] [--model-loader-extra-config MODEL_LOADER_EXTRA_CONFIG] [--served-model-name SERVED_MODEL_NAME [SERVED_MODEL_NAME ...]]
                     [--engine-use-ray] [--disable-log-requests] [--max-log-len MAX_LOG_LEN]

vLLM OpenAI-Compatible RESTful API server.

options:
  -h, --help            show this help message and exit
  --host HOST           host name
  --port PORT           port number
  --uvicorn-log-level {debug,info,warning,error,critical,trace}
                        log level for uvicorn
  --allow-credentials   allow credentials
  --allowed-origins ALLOWED_ORIGINS
                        allowed origins
  --allowed-methods ALLOWED_METHODS
                        allowed methods
  --allowed-headers ALLOWED_HEADERS
                        allowed headers
  --api-key API_KEY     If provided, the server will require this key to be presented in the header.
  --lora-modules LORA_MODULES [LORA_MODULES ...]
                        LoRA module configurations in the format name=path. Multiple modules can be specified.
  --chat-template CHAT_TEMPLATE
                        The file path to the chat template, or the template in single-line form for the specified model
  --response-role RESPONSE_ROLE
                        The role name to return if `request.add_generation_prompt=true`.
  --ssl-keyfile SSL_KEYFILE
                        The file path to the SSL key file
  --ssl-certfile SSL_CERTFILE
                        The file path to the SSL cert file
  --ssl-ca-certs SSL_CA_CERTS
                        The CA certificates file
  --ssl-cert-reqs SSL_CERT_REQS
                        Whether client certificate is required (see stdlib ssl module's)
  --root-path ROOT_PATH
                        FastAPI root_path when app is behind a path based routing proxy
  --middleware MIDDLEWARE
                        Additional ASGI middleware to apply to the app. We accept multiple --middleware arguments. The value should be an import path. If a function is provided, vLLM will add
                        it to the server using @app.middleware('http'). If a class is provided, vLLM will add it to the server using app.add_middleware().
  --model MODEL         Name or path of the huggingface model to use.
  --tokenizer TOKENIZER
                        Name or path of the huggingface tokenizer to use.
  --skip-tokenizer-init
                        Skip initialization of tokenizer and detokenizer
  --revision REVISION   The specific model version to use. It can be a branch name, a tag name, or a commit id. If unspecified, will use the default version.
  --code-revision CODE_REVISION
                        The specific revision to use for the model code on Hugging Face Hub. It can be a branch name, a tag name, or a commit id. If unspecified, will use the default version.
  --tokenizer-revision TOKENIZER_REVISION
                        The specific tokenizer version to use. It can be a branch name, a tag name, or a commit id. If unspecified, will use the default version.
  --tokenizer-mode {auto,slow}
                        The tokenizer mode. * "auto" will use the fast tokenizer if available. * "slow" will always use the slow tokenizer.
  --trust-remote-code   Trust remote code from huggingface.
  --download-dir DOWNLOAD_DIR
                        Directory to download and load the weights, default to the default cache dir of huggingface.
  --load-format {auto,pt,safetensors,npcache,dummy,tensorizer}
                        The format of the model weights to load. * "auto" will try to load the weights in the safetensors format and fall back to the pytorch bin format if safetensors format
                        is not available. * "pt" will load the weights in the pytorch bin format. * "safetensors" will load the weights in the safetensors format. * "npcache" will load the
                        weights in pytorch format and store a numpy cache to speed up the loading. * "dummy" will initialize the weights with random values, which is mainly for profiling. *
                        "tensorizer" will load the weights using tensorizer from CoreWeave which assumes tensorizer_uri is set to the location of the serialized weights.
  --dtype {auto,half,float16,bfloat16,float,float32}
                        Data type for model weights and activations. * "auto" will use FP16 precision for FP32 and FP16 models, and BF16 precision for BF16 models. * "half" for FP16.
                        Recommended for AWQ quantization. * "float16" is the same as "half". * "bfloat16" for a balance between precision and range. * "float" is shorthand for FP32 precision.
                        * "float32" for FP32 precision.
  --kv-cache-dtype {auto,fp8}
                        Data type for kv cache storage. If "auto", will use model data type. FP8_E5M2 (without scaling) is only supported on cuda version greater than 11.8. On ROCm (AMD GPU),
                        FP8_E4M3 is instead supported for common inference criteria.
  --quantization-param-path QUANTIZATION_PARAM_PATH
                        Path to the JSON file containing the KV cache scaling factors. This should generally be supplied, when KV cache dtype is FP8. Otherwise, KV cache scaling factors
                        default to 1.0, which may cause accuracy issues. FP8_E5M2 (without scaling) is only supported on cuda versiongreater than 11.8. On ROCm (AMD GPU), FP8_E4M3 is instead
                        supported for common inference criteria.
  --max-model-len MAX_MODEL_LEN
                        Model context length. If unspecified, will be automatically derived from the model config.
  --guided-decoding-backend {outlines,lm-format-enforcer}
                        Which engine will be used for guided decoding (JSON schema / regex etc) by default. Currently support https://github.com/outlines-dev/outlines and
                        https://github.com/noamgat/lm-format-enforcer. Can be overridden per request via guided_decoding_backend parameter.
  --worker-use-ray      Use Ray for distributed serving, will be automatically set when using more than 1 GPU.
  --pipeline-parallel-size PIPELINE_PARALLEL_SIZE, -pp PIPELINE_PARALLEL_SIZE
                        Number of pipeline stages.
  --tensor-parallel-size TENSOR_PARALLEL_SIZE, -tp TENSOR_PARALLEL_SIZE
                        Number of tensor parallel replicas.
  --max-parallel-loading-workers MAX_PARALLEL_LOADING_WORKERS
                        Load model sequentially in multiple batches, to avoid RAM OOM when using tensor parallel and large models.
  --ray-workers-use-nsight
                        If specified, use nsight to profile Ray workers.
  --block-size {8,16,32}
                        Token block size for contiguous chunks of tokens.
  --enable-prefix-caching
                        Enables automatic prefix caching.
  --use-v2-block-manager
                        Use BlockSpaceMangerV2.
  --num-lookahead-slots NUM_LOOKAHEAD_SLOTS
                        Experimental scheduling config necessary for speculative decoding. This will be replaced by speculative config in the future; it is present to enable correctness tests
                        until then.
  --seed SEED           Random seed for operations.
  --swap-space SWAP_SPACE
                        CPU swap space size (GiB) per GPU.
  --gpu-memory-utilization GPU_MEMORY_UTILIZATION
                        The fraction of GPU memory to be used for the model executor, which can range from 0 to 1. For example, a value of 0.5 would imply 50% GPU memory utilization. If
                        unspecified, will use the default value of 0.9.
  --num-gpu-blocks-override NUM_GPU_BLOCKS_OVERRIDE
                        If specified, ignore GPU profiling result and use this numberof GPU blocks. Used for testing preemption.
  --max-num-batched-tokens MAX_NUM_BATCHED_TOKENS
                        Maximum number of batched tokens per iteration.
  --max-num-seqs MAX_NUM_SEQS
                        Maximum number of sequences per iteration.
  --max-logprobs MAX_LOGPROBS
                        Max number of log probs to return logprobs is specified in SamplingParams.
  --disable-log-stats   Disable logging statistics.
  --quantization {aqlm,awq,fp8,gptq,squeezellm,gptq_marlin,marlin,None}, -q {aqlm,awq,fp8,gptq,squeezellm,gptq_marlin,marlin,None}
                        Method used to quantize the weights. If None, we first check the `quantization_config` attribute in the model config file. If that is None, we assume the model weights
                        are not quantized and use `dtype` to determine the data type of the weights.
  --enforce-eager       Always use eager-mode PyTorch. If False, will use eager mode and CUDA graph in hybrid for maximal performance and flexibility.
  --max-context-len-to-capture MAX_CONTEXT_LEN_TO_CAPTURE
                        Maximum context length covered by CUDA graphs. When a sequence has context length larger than this, we fall back to eager mode. (DEPRECATED. Use --max-seq_len-to-
                        capture instead)
  --max-seq_len-to-capture MAX_SEQ_LEN_TO_CAPTURE
                        Maximum sequence length covered by CUDA graphs. When a sequence has context length larger than this, we fall back to eager mode.
  --disable-custom-all-reduce
                        See ParallelConfig.
  --tokenizer-pool-size TOKENIZER_POOL_SIZE
                        Size of tokenizer pool to use for asynchronous tokenization. If 0, will use synchronous tokenization.
  --tokenizer-pool-type TOKENIZER_POOL_TYPE
                        Type of tokenizer pool to use for asynchronous tokenization. Ignored if tokenizer_pool_size is 0.
  --tokenizer-pool-extra-config TOKENIZER_POOL_EXTRA_CONFIG
                        Extra config for tokenizer pool. This should be a JSON string that will be parsed into a dictionary. Ignored if tokenizer_pool_size is 0.
  --enable-lora         If True, enable handling of LoRA adapters.
  --max-loras MAX_LORAS
                        Max number of LoRAs in a single batch.
  --max-lora-rank MAX_LORA_RANK
                        Max LoRA rank.
  --lora-extra-vocab-size LORA_EXTRA_VOCAB_SIZE
                        Maximum size of extra vocabulary that can be present in a LoRA adapter (added to the base model vocabulary).
  --lora-dtype {auto,float16,bfloat16,float32}
                        Data type for LoRA. If auto, will default to base model dtype.
  --max-cpu-loras MAX_CPU_LORAS
                        Maximum number of LoRAs to store in CPU memory. Must be >= than max_num_seqs. Defaults to max_num_seqs.
  --fully-sharded-loras
                        By default, only half of the LoRA computation is sharded with tensor parallelism. Enabling this will use the fully sharded layers. At high sequence length, max rank or
                        tensor parallel size, this is likely faster.
  --device {auto,cuda,neuron,cpu}
                        Device type for vLLM execution.
  --image-input-type {pixel_values,image_features}
                        The image input type passed into vLLM. Should be one of "pixel_values" or "image_features".
  --image-token-id IMAGE_TOKEN_ID
                        Input id for image token.
  --image-input-shape IMAGE_INPUT_SHAPE
                        The biggest image input shape (worst for memory footprint) given an input type. Only used for vLLM's profile_run.
  --image-feature-size IMAGE_FEATURE_SIZE
                        The image feature size along the context dimension.
  --scheduler-delay-factor SCHEDULER_DELAY_FACTOR
                        Apply a delay (of delay factor multiplied by previousprompt latency) before scheduling next prompt.
  --enable-chunked-prefill
                        If set, the prefill requests can be chunked based on the max_num_batched_tokens.
  --speculative-model SPECULATIVE_MODEL
                        The name of the draft model to be used in speculative decoding.
  --num-speculative-tokens NUM_SPECULATIVE_TOKENS
                        The number of speculative tokens to sample from the draft model in speculative decoding.
  --speculative-max-model-len SPECULATIVE_MAX_MODEL_LEN
                        The maximum sequence length supported by the draft model. Sequences over this length will skip speculation.
  --ngram-prompt-lookup-max NGRAM_PROMPT_LOOKUP_MAX
                        Max size of window for ngram prompt lookup in speculative decoding.
  --ngram-prompt-lookup-min NGRAM_PROMPT_LOOKUP_MIN
                        Min size of window for ngram prompt lookup in speculative decoding.
  --model-loader-extra-config MODEL_LOADER_EXTRA_CONFIG
                        Extra config for model loader. This will be passed to the model loader corresponding to the chosen load_format. This should be a JSON string that will be parsed into a
                        dictionary.
  --served-model-name SERVED_MODEL_NAME [SERVED_MODEL_NAME ...]
                        The model name(s) used in the API. If multiple names are provided, the server will respond to any of the provided names. The model name in the model field of a response
                        will be the first name in this list. If not specified, the model name will be the same as the `--model` argument. Noted that this name(s)will also be used in
                        `model_name` tag content of prometheus metrics, if multiple names provided, metricstag will take the first one.
  --engine-use-ray      Use Ray to start the LLM engine in a separate process as the server process.
  --disable-log-requests
                        Disable logging requests.
  --max-log-len MAX_LOG_LEN
                        Max number of prompt characters or prompt ID numbers being printed in log. Default: Unlimited





USAGE tabbyAPI:

usage: main.py [-h] [--host HOST] [--port PORT] [--disable-auth DISABLE_AUTH] [--model-dir MODEL_DIR] [--model-name MODEL_NAME] [--max-seq-len MAX_SEQ_LEN]
               [--override-base-seq-len OVERRIDE_BASE_SEQ_LEN] [--cache-size CACHE_SIZE] [--rope-scale ROPE_SCALE] [--rope-alpha ROPE_ALPHA] [--prompt-template PROMPT_TEMPLATE]
               [--gpu-split-auto GPU_SPLIT_AUTO] [--gpu-split GPU_SPLIT [GPU_SPLIT ...]] [--num-experts-per-token NUM_EXPERTS_PER_TOKEN] [--use-cfg USE_CFG] [--log-prompt LOG_PROMPT]
               [--log-generation-params LOG_GENERATION_PARAMS] [--unsafe-launch UNSAFE_LAUNCH] [--disable-request-streaming DISABLE_REQUEST_STREAMING]
               [--cuda-malloc-backend CUDA_MALLOC_BACKEND] [--config CONFIG]

options:
  -h, --help            show this help message and exit
  --config CONFIG       Path to an overriding config.yml file

network:
  --host HOST           The IP to host on
  --port PORT           The port to host on
  --disable-auth DISABLE_AUTH
                        Disable HTTP token authenticaion with requests

model:
  --model-dir MODEL_DIR
                        Overrides the directory to look for models
  --model-name MODEL_NAME
                        An initial model to load
  --max-seq-len MAX_SEQ_LEN
                        Override the maximum model sequence length
  --override-base-seq-len OVERRIDE_BASE_SEQ_LEN
                        Overrides base model context length
  --cache-size CACHE_SIZE
                        The size of the prompt cache (in number of tokens) to allocate
  --rope-scale ROPE_SCALE
                        Sets rope_scale or compress_pos_emb
  --rope-alpha ROPE_ALPHA
                        Sets rope_alpha for NTK
  --prompt-template PROMPT_TEMPLATE
                        Set the prompt template for chat completions
  --gpu-split-auto GPU_SPLIT_AUTO
                        Automatically allocate resources to GPUs
  --gpu-split GPU_SPLIT [GPU_SPLIT ...]
                        An integer array of GBs of vram to split between GPUs. Ignored if gpu_split_auto is true
  --num-experts-per-token NUM_EXPERTS_PER_TOKEN
                        Number of experts to use per token in MoE models
  --use-cfg USE_CFG     Enables CFG support

logging:
  --log-prompt LOG_PROMPT
                        Enable prompt logging
  --log-generation-params LOG_GENERATION_PARAMS
                        Enable generation parameter logging

developer:
  --unsafe-launch UNSAFE_LAUNCH
                        Skip Exllamav2 version check
  --disable-request-streaming DISABLE_REQUEST_STREAMING
                        Disables API request streaming
  --cuda-malloc-backend CUDA_MALLOC_BACKEND
                        Disables API request streaming

